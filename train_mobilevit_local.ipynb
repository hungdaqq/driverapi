{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('models')"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["from mobilevitv1 import MobileViT\n","from mobilevitv2 import MobileViTv2"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["image_size = (256,256)\n","num_classes = 10\n","batch_size = 16\n","num_epochs = 50\n","lr = 1e-3"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["from torchvision import transforms\n","# Additional augmentations\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n","    transforms.RandomRotation(degrees=10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n","    transforms.RandomGrayscale(p=0.1),  # Randomly convert images to grayscale\n","    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n","    # transforms.Resize(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split\n","\n","# Define the path to your dataset\n","train_path = '/home/hung/auc.distracted.driver.dataset_v2/v2/cam2/train/'\n","test_path = '/home/hung/auc.distracted.driver.dataset_v2/v2/cam2/test/'\n","\n","# Create the ImageFolder dataset\n","train_dataset = ImageFolder(root=train_path, transform=transform)\n","test_dataset = ImageFolder(root=test_path, transform=transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model architecture (make sure it matches the one used during training)\n","\n","# model = MobileViTv2(\n","#     image_size = image_size, \n","#     width_multiplier = 0.5,             # support [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2] as shown in paper\n","#     num_classes=num_classes, \n","#     patch_size=(2, 2)\n","# )\n","\n","# model = models.resnet50(pretrained=True)\n","# model.fc = nn.Sequential(\n","#     nn.Dropout(p=0.2, inplace=False),\n","#     nn.Linear(in_features=model.fc.in_features, out_features=num_cls, bias=True)\n","# )\n","\n","model = MobileViT(\n","    image_size=(256,256),\n","    mode='xx_small',\n","    num_classes=num_classes,\n","    patch_size=(2,2)\n",")\n","pretrained = 'pretrained/mobilevit_xxs.pt'\n","state_dict = torch.load(pretrained, map_location=device)\n","# Update the classifier weights and biases in the state dict\n","state_dict['classifier.fc.weight'] = torch.randn(num_classes, model.classifier.fc.in_features)\n","state_dict['classifier.fc.bias'] = torch.randn(num_classes)\n","model.load_state_dict(state_dict)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","# Assuming you have a list to store your logs\n","training_logs = []"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Oct 30 16:41:20 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n","| N/A   42C    P0    16W /  50W |    856MiB /  4096MiB |     29%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1111      G   /usr/lib/xorg/Xorg                159MiB |\n","|    0   N/A  N/A      1439      G   /usr/bin/gnome-shell               41MiB |\n","|    0   N/A  N/A     11034      G   ...RendererForSitePerProcess      105MiB |\n","|    0   N/A  N/A     12318    C+G   ...193504807944160081,262144       69MiB |\n","|    0   N/A  N/A     12632      C   ...VHT/driver/env/bin/python      474MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# Train the model\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total_samples += labels.size(0)\n","        total_correct += (predicted == labels).sum().item()\n","        \n","        print(f'Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(train_loader)}, '\n","              f'Train Loss: {total_loss:.4f}, Train Acc: {total_correct}', end='\\r')\n","        \n","    scheduler.step()\n","    train_loss = total_loss / len(train_loader)\n","    train_accuracy = total_correct / total_samples\n","\n","    # Validation\n","    model.eval()\n","    total_correct = 0\n","    total_samples = 0\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for step, (inputs, labels) in enumerate(val_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total_samples += labels.size(0)\n","            total_correct += (predicted == labels).sum().item()\n","\n","            print(f'Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(val_loader)}, '\n","              f'Val Loss: {val_loss:.4f}, Val Acc: {total_correct}', end='\\r')\n","\n","    val_loss /= len(val_loader)\n","    val_accuracy = total_correct / total_samples\n","    \n","    log_entry = {\n","        'epoch': epoch + 1,\n","        'train_loss': train_loss,\n","        'train_accuracy': train_accuracy,\n","        'val_loss': val_loss,\n","        'val_accuracy': val_accuracy,\n","        'learning_rate': optimizer.param_groups[0][\"lr\"]\n","    }\n","\n","    training_logs.append(log_entry)\n","    \n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n","          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}',\n","          f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the logs to a JSON file\n","with open('training_logs.json', 'w') as json_file:\n","    json.dump(training_logs, json_file)\n","# Save the trained model\n","torch.save(model.state_dict(), f'mobilevit_{num_epochs}.pt')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
