{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('models')"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["from mobilevitv1 import MobileViT\n","from mobilevitv2 import MobileViTv2"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["image_size = (232,232)\n","num_classes = 10\n","batch_size = 16\n","num_epochs = 50\n","lr = 1e-3"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["from torchvision import transforms\n","# Additional augmentations\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n","    transforms.RandomRotation(degrees=10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n","    transforms.RandomGrayscale(p=0.1),\n","    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n","    # transforms.Resize(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split\n","\n","# Define the path to your dataset\n","# train_path = '/home/hung/auc.distracted.driver.dataset_v2/v2/cam1/train/'\n","# test_path = '/home/hung/auc.distracted.driver.dataset_v2/v2/cam1/test/'\n","train_path = '/home/hung/auc.distracted.driver.dataset_v2/v1/train/'\n","test_path = '/home/hung/auc.distracted.driver.dataset_v2/v1/test/'\n","\n","# Create the ImageFolder dataset\n","train_dataset = ImageFolder(root=train_path, transform=transform)\n","test_dataset = ImageFolder(root=test_path, transform=transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn \n","from torchvision import models\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model architecture (make sure it matches the one used during training)\n","\n","# model = models.resnet50(pretrained=True)\n","# model.fc = nn.Sequential(\n","#     nn.Dropout(p=0.2, inplace=False),\n","#     nn.Linear(in_features=model.fc.in_features, out_features=num_cls, bias=True)\n","# )\n","\n","# model = models.mnasnet0_75(weights=True)\n","# model.classifier = nn.Sequential(\n","#     nn.Dropout(p=0.2, inplace=True),\n","#     nn.Linear(in_features=model.classifier[1].in_features, out_features=num_classes, bias=True)\n","# )\n","# print(model)\n","\n","model = MobileViTv2(\n","    image_size = image_size, \n","    width_multiplier = 0.5,             # support [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2] as shown in paper\n","    num_classes=1000, \n","    patch_size=(2, 2)\n",")\n","pretrained = 'pretrained/mobilevitv2-0.5.pt'\n","state_dict = torch.load(pretrained, map_location=device)\n","model.load_state_dict(state_dict)\n","model.classifier = nn.Sequential(\n","    nn.Linear(in_features=model.classifier[0].in_features, out_features=num_classes,bias=True)\n",")\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","# Assuming you have a list to store your logs\n","training_logs = []"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Nov  1 18:07:47 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n","| N/A   41C    P0    16W /  50W |    926MiB /  4096MiB |     20%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1145      G   /usr/lib/xorg/Xorg                153MiB |\n","|    0   N/A  N/A      1449      G   /usr/bin/gnome-shell               70MiB |\n","|    0   N/A  N/A      2087    C+G   ...368805767241554748,262144      125MiB |\n","|    0   N/A  N/A      2643      G   ...RendererForSitePerProcess      114MiB |\n","|    0   N/A  N/A      3130      C   ...VHT/driver/env/bin/python      456MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50, Train Loss: 1.9580, Train Accuracy: 0.3014, Val Loss: 2.8168, Val Accuracy: 0.1011 Learning Rate: 0.000900\n","Epoch 2/50, Train Loss: 1.5404, Train Accuracy: 0.4737, Val Loss: 14.9249, Val Accuracy: 0.0704 Learning Rate: 0.000810\n","Epoch 3/50, Train Loss: 1.4292, Train Accuracy: 0.5221, Val Loss: 9.5386, Val Accuracy: 0.2669 Learning Rate: 0.000729\n","Epoch 4/50, Train Loss: 1.3218, Train Accuracy: 0.5635, Val Loss: 21.6727, Val Accuracy: 0.0750 Learning Rate: 0.000656\n","Epoch 5/50, Step 13/812, Train Loss: 16.2731, Train Acc: 120\r"]}],"source":["# Train the model\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total_samples += labels.size(0)\n","        total_correct += (predicted == labels).sum().item()\n","        \n","#         print(f'Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(train_loader)}, '\n","#               f'Train Loss: {total_loss:.4f}, Train Acc: {total_correct}', end='\\r')\n","        \n","    scheduler.step()\n","    train_loss = total_loss / len(train_loader)\n","    train_accuracy = total_correct / total_samples\n","\n","    # Validation\n","    model.eval()\n","    total_correct = 0\n","    total_samples = 0\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for step, (inputs, labels) in enumerate(val_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total_samples += labels.size(0)\n","            total_correct += (predicted == labels).sum().item()\n","\n","#             print(f'Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(val_loader)}, '\n","#               f'Val Loss: {val_loss:.4f}, Val Acc: {total_correct}', end='\\r')\n","\n","    val_loss /= len(val_loader)\n","    val_accuracy = total_correct / total_samples\n","    \n","    log_entry = {\n","        'epoch': epoch + 1,\n","        'train_loss': train_loss,\n","        'train_accuracy': train_accuracy,\n","        'val_loss': val_loss,\n","        'val_accuracy': val_accuracy,\n","        'learning_rate': optimizer.param_groups[0][\"lr\"]\n","    }\n","\n","    training_logs.append(log_entry)\n","    \n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n","          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}',\n","          f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the logs to a JSON file\n","with open('training_logs.json', 'w') as json_file:\n","    json.dump(training_logs, json_file)\n","# Save the trained model\n","torch.save(model.state_dict(), f'mobilevit_{num_epochs}.pt')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
